{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "import json\n",
    "import dateutil\n",
    "import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import BulkWriteError, DuplicateKeyError\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import string\n",
    "import functools\n",
    "from spacy.en import English\n",
    "import textacy\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from multiprocessing.dummy import Pool\n",
    "import pickle\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper Implementation\n",
    "[\n",
    "Deep learning for event-driven stock prediction](http://dl.acm.org/citation.cfm?id=2832415.2832572)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGO_URL=None\n",
    "MONGO_USERNAME=None\n",
    "MONGO_PASSWORD=None\n",
    "# add your servers here if you want\n",
    "STANFORD_SERVERS=[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(MONGO_URL)\n",
    "db = client.stockdb\n",
    "db.authenticate(name=MONGO_USERNAME, password=MONGO_PASSWORD)\n",
    "news_coll=db.news_latest\n",
    "stock_coll=db.stockcoll\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanford=StanfordCoreNLP('http://127.0.0.1:9000')\n",
    "stanford_server_list=[stanford]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "samples=list(news_coll.find({'content': {'$regex':'(Apple\\s|Google|Amazon|Microsoft|Facebook|IBM|Twitter|Nvidia)'}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23205"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dates=list(sorted(list(set(map(lambda x:x['date'],samples)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies=pd.read_csv('https://datahub.io/core/s-and-p-500-companies-financials/r/constituents-financials.csv')\n",
    "companies.columns=list(map(lambda x:x.strip().lower(),companies.columns))\n",
    "# companies=companies[companies.symbol.isin(['GOOGL','IBM','ORCL','AAPL','YHOO','FB'])]\n",
    "companies.index=companies['_id']\n",
    "companies=companies[['symbol','name','sector']]\n",
    "company_names=companies['name'].values\n",
    "company_symbols=companies['symbol'].values\n",
    "company_info=companies[['symbol','name','name']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_company_name=['&','the','company','inc','inc.','plc','corp','corp.','co','co.','worldwide','corporation','group','']\n",
    "# stop_company_name=[]\n",
    "splitted_companies=list(map(lambda x:([x[0]]+[x[1]]+list(filter(lambda y: y.lower() not in stop_company_name ,x[2].split(' ')))),company_info))\n",
    "splitted_companies=list(map(lambda x:[x[0]]+[x[1]]+[re.sub(pattern='[^.a-zA-Z0-9\\s-]',repl='',string=functools.reduce(lambda y,z:y+' '+z,x[2:]))],splitted_companies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_extractor(text,server):\n",
    "    doc=nlp(text)\n",
    "    svo_triples=list(textacy.extract.subject_verb_object_triples(doc))\n",
    "    str_triples=map(lambda x:(str(x[0]),str(x[1]),str(x[2])) if len(x)>0 else _,svo_triples)\n",
    "    return svo_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corenlp_extractor(text,debug=False,server=0):\n",
    "    svo_triples=[]\n",
    "    output=stanford_server_list[server].annotate(text, properties={\n",
    "        'annotators': 'openie',\n",
    "        'outputFormat': 'json'\n",
    "        })\n",
    "    if type(output)==dict:\n",
    "        for sentence in output['sentences']:\n",
    "            svo_triples.extend(list(map(lambda x: (x['subject'],x['relation'],x['object']),sentence['openie'])))\n",
    "    return svo_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_SVO_embedding(text,word_embedding):\n",
    "    return np.array([ word_embedding[str(term)] for term in list(text) ]).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def core_nlp_to_SVO_embedding(text,word_embedding):\n",
    "    embeddings=[ word_embedding.wv[str(term)] if str(term) in word_embedding.wv else None for term in list(nlp(text))]\n",
    "    embeddings=list(filter(lambda x: x is not None,embeddings))\n",
    "    if len(embeddings)==0:\n",
    "        return None\n",
    "    return np.array(embeddings).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.append('would')\n",
    "stopwords.append('kmh')\n",
    "stopwords.append('mph')\n",
    "stopwords.append('u')\n",
    "stopwords.extend(list(string.ascii_lowercase))\n",
    "stop_symbols=['JAN','FEB','MAR','APR','MAY','JUN','JUL','AUG','SEP','OCT','NOV','DEC','MON','TUE','WED','THU','FRI','SAT','SUN',\n",
    "              'company','Inc','Company','inc','Inc.','plc','Plc','corp','Corp.','Co','co','Co.','co.','worldwide','corporation','group']\n",
    "regex = re.compile(r'[^A-Za-z-]')\n",
    "def extract_events_and_companies_mp(sample_server,companies,word_embedding,events_extractor=spacy_extractor,**kwargs):\n",
    "    processed_data=[]\n",
    "    sample=sample_server[0]\n",
    "    server=sample_server[1]\n",
    "    sample_date=sample['date']\n",
    "    text=sample['content']\n",
    "    processed_data=None\n",
    "    svo_triples=events_extractor(text,server=server,**kwargs)\n",
    "    client = MongoClient(MONGO_URL)\n",
    "    db = client.stockdb\n",
    "    db.authenticate(name=MONGO_USERNAME, password=MONGO_PASSWORD)\n",
    "    news_coll=db.news_latest\n",
    "    stock_coll=db.stockcoll\n",
    "    company_coll=db.sp500company\n",
    "    if len(svo_triples) == 0:\n",
    "        return processed_data\n",
    "    svo_result=[]\n",
    "    for svo in svo_triples:\n",
    "        if len(svo) == 0:\n",
    "            return processed_data\n",
    "        o1=svo[0]\n",
    "        p=svo[1]\n",
    "        o2=svo[2]\n",
    "        if events_extractor==spacy_extractor:\n",
    "            svo_embedding=(to_SVO_embedding(o1,word_embedding),to_SVO_embedding(p,word_embedding),to_SVO_embedding(o2,word_embedding))\n",
    "        else:\n",
    "            svo_embedding=(core_nlp_to_SVO_embedding(o1,word_embedding),core_nlp_to_SVO_embedding(p,word_embedding),core_nlp_to_SVO_embedding(o2,word_embedding))\n",
    "        if svo_embedding[0] is None or svo_embedding[1] is None or svo_embedding[2] is None:\n",
    "            return processed_data\n",
    "        o1=str(o1)\n",
    "        p=str(p)\n",
    "        o2=str(o2)\n",
    "        got=False\n",
    "        for company in companies:\n",
    "            if got == True: \n",
    "                break\n",
    "            for cpy in company:\n",
    "                if len(o1)>2 and o1 not in stop_symbols and o1 in cpy:\n",
    "                    future_price_data=list(stock_coll.find({'symbol':company[0],'date':{'$gte':sample_date}}).limit(2))\n",
    "                    past_price_data=pd.DataFrame(list(stock_coll.find({'symbol':company[0],'date':{'$lte':sample_date}}).sort('date',-1).limit(7)))\n",
    "                    if len(past_price_data)!=7:break\n",
    "                    past_price_data=scale(past_price_data['adj_close'].values[0:-1]-past_price_data['adj_close'].values[1:])\n",
    "                    if len(future_price_data)<2:break\n",
    "                    if (future_price_data[0]['date']-sample_date).days>2: break\n",
    "                    price_label=np.sign(future_price_data[1]['adj_close']-future_price_data[0]['adj_close'])\n",
    "                    svo_result.append((svo,svo_embedding,company,price_label,past_price_data,sample_date))\n",
    "                    got=True\n",
    "                    break\n",
    "                if len(p)>2 and o1 not in stop_symbols and p in cpy:\n",
    "                    future_price_data=list(stock_coll.find({'symbol':company[0],'date':{'$gte':sample_date}}).limit(2))\n",
    "                    past_price_data=pd.DataFrame(list(stock_coll.find({'symbol':company[0],'date':{'$lte':sample_date}}).sort('date',-1).limit(7)))\n",
    "                    if len(past_price_data)!=7:break\n",
    "                    past_price_data=scale(past_price_data['adj_close'].values[0:-1]-past_price_data['adj_close'].values[1:])\n",
    "                    if len(future_price_data)<2:break\n",
    "                    if (future_price_data[0]['date']-sample_date).days>2: break\n",
    "                    price_label=np.sign(future_price_data[1]['adj_close']-future_price_data[0]['adj_close'])\n",
    "                    svo_result.append((svo,svo_embedding,company,price_label,past_price_data,sample_date))\n",
    "                    got=True\n",
    "                    break\n",
    "                if len(o2)>2 and o2 not in stop_symbols and o2 in cpy:\n",
    "                    future_price_data=list(stock_coll.find({'symbol':company[0],'date':{'$gte':sample_date}}).limit(3))\n",
    "                    past_price_data=pd.DataFrame(list(stock_coll.find({'symbol':company[0],'date':{'$lte':sample_date}}).sort('date',-1).limit(7)))\n",
    "                    if len(past_price_data)!=7:break\n",
    "                    past_price_data=scale(past_price_data['adj_close'].values[0:-1]-past_price_data['adj_close'].values[1:])\n",
    "                    if len(future_price_data)<2:break\n",
    "                    if (future_price_data[0]['date']-sample_date).days>3: break\n",
    "                    price_label=np.sign(future_price_data[1]['adj_close']-future_price_data[0]['adj_close'])\n",
    "                    svo_result.append((svo,svo_embedding,company,price_label,past_price_data,sample_date))\n",
    "                    got=True\n",
    "                    break\n",
    "    if len(svo_result)>0:\n",
    "        processed_data={'result':svo_result}\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "def extract_all_events_for_company_mp(sample_server,company,word_embedding,events_extractor=spacy_extractor,**kwargs):\n",
    "    processed_data=[]\n",
    "    sample=sample_server[0]\n",
    "    server=sample_server[1]\n",
    "    sample_date=sample['date']\n",
    "    text=sample['content']\n",
    "    processed_data=None\n",
    "    future_price_data=list(stock_coll.find({'symbol':company,'date':{'$gte':sample_date}}).limit(2))\n",
    "    past_price_data=pd.DataFrame(list(stock_coll.find({'symbol':company,'date':{'$lte':sample_date}}).sort('date',-1).limit(7)))\n",
    "    if len(past_price_data)!=7:return processed_data\n",
    "    past_price_data=scale(past_price_data['adj_close'].values[0:-1]-past_price_data['adj_close'].values[1:])\n",
    "    if len(future_price_data)<2:return processed_data\n",
    "    if (future_price_data[0]['date']-sample_date).days>2: return processed_data\n",
    "    price_label=np.sign(future_price_data[1]['adj_close']-future_price_data[0]['adj_close'])\n",
    "    svo_triples=events_extractor(text,server=server,**kwargs)\n",
    "    if len(svo_triples) == 0:\n",
    "        return processed_data\n",
    "    svo_result=[]\n",
    "    for svo in svo_triples:\n",
    "        if len(svo) == 0:\n",
    "            continue\n",
    "        o1=svo[0]\n",
    "        p=svo[1]\n",
    "        o2=svo[2]\n",
    "        svo_embedding=[core_nlp_to_SVO_embedding(o1,word_embedding),core_nlp_to_SVO_embedding(p,word_embedding),core_nlp_to_SVO_embedding(o2,word_embedding)]\n",
    "        if svo_embedding[0] is None or svo_embedding[1] is None or svo_embedding[2] is None:\n",
    "            continue\n",
    "        svo_result.append(np.array(svo_embedding))\n",
    "    if len(svo_result)>0:\n",
    "        processed_data=[np.array(svo_result),price_label,past_price_data,sample_date]\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_index=(list(range(stanford_server_list))*2000)[:len(samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_server=list(zip(samples,server_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sample_to_sentences(sample):\n",
    "    return list(map(lambda x: str(x),list(nlp(sample['content'])) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=nlp(samples[0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 15s, sys: 58.4 s, total: 11min 13s\n",
      "Wall time: 2min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pool=Pool(8)\n",
    "sentences=pool.map(process_sample_to_sentences,samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23205"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 47s, sys: 696 ms, total: 9min 48s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_embedding=Word2Vec(sentences,min_count=1,workers=8,size=150,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_result=[]\n",
    "test_result=extract_events_and_companies_mp(sample_server[0],[['AAPL', 'Apple Inc.', 'Apple']],word_embedding,events_extractor=corenlp_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_result=[]\n",
    "test_result=extract_all_events_for_company_mp(sample_server[0],'AAPL',word_embedding,events_extractor=corenlp_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multiple companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 8s, sys: 24.9 s, total: 7min 33s\n",
      "Wall time: 13min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pool=Pool(8)\n",
    "partial_work = partial(extract_events_and_companies_mp,companies=[['GOOG', 'Alphabet Inc', 'Google']],word_embedding=word_embedding,events_extractor=corenlp_extractor) \n",
    "test_result = pool.map(partial_work, sample_server)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For single company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 12s, sys: 34.6 s, total: 9min 47s\n",
      "Wall time: 15min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pool=Pool(8)\n",
    "partial_work = partial(extract_all_events_for_company_mp,company='AAPL',word_embedding=word_embedding,events_extractor=corenlp_extractor) \n",
    "test_result = pool.map(partial_work, sample_server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_result=list(filter(lambda x: x is not None, test_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7669"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Neural Tensor Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the script in models to train NTN and get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "svo_embeddings=tuple([ result[0] for result in final_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "events=np.vstack(tup=svo_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('extracted_data','wb') as f:\n",
    "    pickle.dump(final_result,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data',np.array(events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('data.npy')\n",
    "O1_ndarray = data[:, 0, :]\n",
    "P_ndarray = data[:, 1, :]\n",
    "O2_ndarray = data[:, 2, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_result=np.load('tmp_result.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(717423, 100)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=embedding_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result_dict','rb') as f:\n",
    "    embedding_dict=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index=np.array(sorted(embedding_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label=[ label[3] for result in final_result for label in result['result']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label=np.array(y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=embedding_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "O1_ndarray = events[:, 0, :]\n",
    "P_ndarray = events[:, 1, :]\n",
    "O2_ndarray = events[:, 2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.hstack((O1_ndarray,P_ndarray,O2_ndarray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array([ result[1] for result in final_result for r in result[0]])\n",
    "price_sequence=np.array([ result[2] for result in final_result for _ in result[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.hstack((X,price_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import regularizers, optimizers\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dates=list(map(lambda x:x[3],final_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_label=list(map(lambda x:x['result'][0][3],final_result))\n",
    "event_price=list(map(lambda x:x['result'][0][4],final_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dates=list(sorted(list(set(event_dates))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=[]\n",
    "tmp_index=0\n",
    "for result in final_result:\n",
    "    data_in_one_doc=[]\n",
    "    for event in result['result']:\n",
    "#         date=event[5]\n",
    "        label=event[3]\n",
    "        price=event[4]\n",
    "        embedding=embedding_result[tmp_index]\n",
    "        feature_vector=np.concatenate((embedding,price,np.array([label])))\n",
    "        data_in_one_doc.append(feature_vector)\n",
    "        tmp_index+=1\n",
    "    all_data.append(np.mean(data_in_one_doc,axis=0))\n",
    "x=np.array(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates=list(sorted(list(set(event_dates)),reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=[]\n",
    "\n",
    "for date in all_dates:\n",
    "    data_in_one_day=[]\n",
    "    tmp_index=0\n",
    "    for result in final_result:\n",
    "        for event in result['result']:\n",
    "            event_date=event[5]\n",
    "            if date==event_date:\n",
    "                label=event[3]\n",
    "                price=event[4]\n",
    "                embedding=embedding_result[tmp_index]\n",
    "                feature_vector=np.concatenate((embedding,price,np.array([label])))\n",
    "                data_in_one_day.append(feature_vector)\n",
    "            tmp_index+=1\n",
    "    all_data.append(np.mean(data_in_one_day,axis=0)) \n",
    "x=np.array(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label=np.array(list(map(lambda x: np.array([0,1]) if x>0 else np.array([1,0]),y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=embedding_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array([ label[3] for result in final_result for label in result['result']])\n",
    "y[y==-1]=0\n",
    "y_label=np.array(list(map(lambda x: np.array([0,1]) if x>0 else np.array([1,0]),y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 502196 samples, validate on 215227 samples\n",
      "Epoch 1/10\n",
      "502196/502196 [==============================] - 9s - loss: 0.6833 - acc: 0.5540 - val_loss: 0.6979 - val_acc: 0.4951\n",
      "Epoch 2/10\n",
      "502196/502196 [==============================] - 9s - loss: 0.6662 - acc: 0.5823 - val_loss: 0.6953 - val_acc: 0.4903\n",
      "Epoch 3/10\n",
      "502196/502196 [==============================] - 9s - loss: 0.6599 - acc: 0.5883 - val_loss: 0.6948 - val_acc: 0.4899\n",
      "Epoch 4/10\n",
      "502196/502196 [==============================] - 8s - loss: 0.6570 - acc: 0.5915 - val_loss: 0.6962 - val_acc: 0.4887\n",
      "Epoch 5/10\n",
      "502196/502196 [==============================] - 9s - loss: 0.6551 - acc: 0.5927 - val_loss: 0.6947 - val_acc: 0.5046\n",
      "Epoch 6/10\n",
      "502196/502196 [==============================] - 9s - loss: 0.6543 - acc: 0.5919 - val_loss: 0.6944 - val_acc: 0.4961\n",
      "Epoch 7/10\n",
      "502196/502196 [==============================] - 8s - loss: 0.6529 - acc: 0.5938 - val_loss: 0.6952 - val_acc: 0.5093\n",
      "Epoch 8/10\n",
      "502196/502196 [==============================] - 9s - loss: 0.6521 - acc: 0.5930 - val_loss: 0.6946 - val_acc: 0.5006\n",
      "Epoch 9/10\n",
      "502196/502196 [==============================] - 8s - loss: 0.6512 - acc: 0.5941 - val_loss: 0.6948 - val_acc: 0.4969\n",
      "Epoch 10/10\n",
      "502196/502196 [==============================] - 8s - loss: 0.6505 - acc: 0.5942 - val_loss: 0.6947 - val_acc: 0.5079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f42193f5278>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# model.add(Flatten(input_shape=(x.shape, input_dim)))\n",
    "model.add(Dense(32, input_dim=100,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# process Training\n",
    "model.fit(X, y_label, batch_size=128, verbose=1,validation_split=0.3, epochs=10,shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "import json\n",
    "import dateutil\n",
    "import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import BulkWriteError, DuplicateKeyError\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import numpy as np\n",
    "import string\n",
    "import functools\n",
    "import spacy\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper Implementation\n",
    "[Leverage Financial News to Predict Stock Price Movements Using Word Embeddings and Deep Neural Networks](http://aclweb.org/anthology/N16-1041)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure that you have already feed news data and price data into mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGO_URL=None\n",
    "MONGO_USERNAME=None\n",
    "MONGO_PASSWORD=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(MONGO_URL)\n",
    "db = client.stockdb\n",
    "db.authenticate(name=MONGO_USERNAME, password=MONGO_PASSWORD)\n",
    "news_coll=db.news_latest\n",
    "stock_coll=db.stockcoll\n",
    "company_coll=db.sp500company\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=list(news_coll.find().limit(50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies=pd.read_csv('https://datahub.io/core/s-and-p-500-companies-financials/r/constituents-financials.csv')\n",
    "companies.columns=list(map(lambda x:x.strip().lower(),companies.columns))\n",
    "# companies=companies[companies.symbol.isin(['GOOGL','IBM','ORCL','AAPL','YHOO','FB'])]\n",
    "companies.index=companies['symbol']\n",
    "companies=companies[['symbol','name','sector']]\n",
    "company_names=companies['name'].values\n",
    "company_symbols=companies['symbol'].values\n",
    "company_info=companies[['symbol','name','name']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_company_name=['&','the','company','inc','inc.','plc','corp','corp.','co','co.','worldwide','corporation','group','']\n",
    "# stop_company_name=[]\n",
    "splitted_companies=list(map(lambda x:([x[0]]+[x[1]]+list(filter(lambda y: y.lower() not in stop_company_name ,x[2].split(' ')))),company_info))\n",
    "splitted_companies=list(map(lambda x:[x[0]]+[x[1]]+[re.sub(pattern='[^a-zA-Z0-9\\s-]',repl='',string=functools.reduce(lambda y,z:y+' '+z,x[2:]))],splitted_companies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_remove_stopwords_extract_companies_with_spacy(text,sample_date,companies):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stopwords.append('would')\n",
    "    stopwords.append('kmh')\n",
    "    stopwords.append('mph')\n",
    "    stopwords.append('u')\n",
    "    stopwords.extend(list(string.ascii_lowercase))\n",
    "    stop_symbols=['JAN','FEB','MAR','APR','MAY','JUN','JUL','AUG','SEP','OCT','NOV','DEC','MON','TUE','WED','THU','FRI','SAT','SUN']\n",
    "    processed_data=[]\n",
    "    regex = re.compile(r'[^A-Za-z-]')\n",
    "    doc=nlp(text)\n",
    "    sentences=list(doc.sents)\n",
    "    for sentence in sentences:\n",
    "        tokens=list(map(str,sentence))\n",
    "        complete_sentence=str(sentence)\n",
    "        sent_doc=nlp(complete_sentence)\n",
    "        entities=list(map(str,sent_doc.ents))\n",
    "        for company in companies:\n",
    "            if company[1] in entities or company[2] in complete_sentence or company[0] in entities :\n",
    "                future_price_data=list(stock_coll.find({'symbol':company[0],'date':{'$gte':sample_date}}).limit(2))\n",
    "                past_price_data=pd.DataFrame(list(stock_coll.find({'symbol':company[0],'date':{'$lte':sample_date}}).sort('date',-1).limit(7)))\n",
    "                if len(past_price_data)!=7:continue\n",
    "                past_price_data=scale(past_price_data['adj_close'].values[0:-1]-past_price_data['adj_close'].values[1:])\n",
    "                if len(future_price_data)<2:continue\n",
    "                if (future_price_data[0]['date']-sample_date).days>3: continue\n",
    "                price_label=np.sign(future_price_data[1]['adj_close']-future_price_data[0]['adj_close'])\n",
    "                processed_data.append((complete_sentence,tokens,sent_doc,company[0],company[1],company[2],price_label,past_price_data,sample_date))\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_samples=[]\n",
    "for sample in samples:\n",
    "    p_sample=tokenize_remove_stopwords_extract_companies_with_spacy(sample['content'],sample['date'],splitted_companies)\n",
    "    if len(p_sample)==0:continue\n",
    "    p_sample=np.array(p_sample)\n",
    "    processed_samples.extend(p_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36830"
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import _thread\n",
    "# import threading\n",
    "# processed_samples=[]\n",
    "# class Processor(threading.Thread):\n",
    "#     def __init__(self, samples,companies):\n",
    "#         threading.Thread.__init__(self)\n",
    "#         self.samples = samples\n",
    "#         self.companies=companies\n",
    "#     def run(self):\n",
    "#         for sample in self.samples:\n",
    "#             processed_sample=tokenize_remove_stopwords_extract_companies(sample['content'],sample['date'],self.companies)\n",
    "#             if len(processed_sample)==0:continue\n",
    "#             processed_sample=np.array(processed_sample)\n",
    "#             processed_samples.extend(processed_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_samples=[]\n",
    "# for i in range(8):\n",
    "#     processer=Processor(samples=samples[i*int(len(samples)/8):(i+1)*int(len(samples)/8)],companies=splitted_companies)\n",
    "#     processer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_samples=np.array(processed_samples)\n",
    "sentences=processed_samples[:,0]\n",
    "sentences_terms=processed_samples[:,1]\n",
    "sentence_handler=processed_samples[:,2]\n",
    "labels=processed_samples[:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model=Word2Vec(sentences=sentences_terms,min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bag_of_keywords=set(['rise','drop','fall','gain','surge','shrink','jump','slump'])\n",
    "stop=False\n",
    "bok_size=1000\n",
    "for i in range(10):\n",
    "    new_words=[]\n",
    "    if stop:break\n",
    "    for k in bag_of_keywords:\n",
    "        if k in model.wv.vocab.keys():\n",
    "            new_words.extend(model.most_similar(k))\n",
    "    for n in new_words:\n",
    "        if n[0].islower() and len(n[0])>3 and n[0].isalpha():\n",
    "            bag_of_keywords.add(n[0])\n",
    "            if len(bag_of_keywords)==bok_size:\n",
    "                stop=True\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_keywords=np.array(list(bag_of_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "bok_tfidf=TfidfVectorizer(lowercase=False,min_df=1,use_idf=True,vocabulary=bag_of_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bok_tfidf=bok_tfidf.fit_transform(sentences)\n",
    "X_bok_tfidf=X_bok_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "bok_count=CountVectorizer(lowercase=False,min_df=1,vocabulary=bag_of_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bok_count=bok_count.fit_transform(sentences)\n",
    "X_bok_count=X_bok_count.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bok_freq_w_pos=np.array(np.sum(X_bok_count[labels[:,3]==1.0],axis=0)*X_bok_count.shape[0]).reshape(X_bok_count.shape[1],)\n",
    "bok_freq_w_pos[bok_freq_w_pos==0]=1\n",
    "bok_freq_w=np.sum(X_bok_count,axis=0)\n",
    "bok_freq_pos=np.sum(labels[:,3]==1.0)\n",
    "bok_PMI_pos=np.log(bok_freq_w_pos*sentences.shape[0]/bok_freq_w*bok_freq_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "bok_freq_w_neg=np.array(np.sum(X_bok_count[labels[:,3]==-1.0],axis=0)*X_bok_count.shape[0]).reshape(X_bok_count.shape[1],)\n",
    "bok_freq_w_neg[bok_freq_w_neg==0]=1\n",
    "bok_freq_w=np.sum(X_bok_count,axis=0)\n",
    "bok_freq_neg=np.sum(labels[:,3]==-1.0)\n",
    "bok_PMI_neg=np.log(bok_freq_w_neg*sentences.shape[0]/bok_freq_w*bok_freq_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "bok_PS=bok_PMI_pos-bok_PMI_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_tags=set(['published','presented','unveil','investment','bankrupt','government','acquisition','suit'])\n",
    "stop=False\n",
    "cate_size=1000\n",
    "for i in range(10):\n",
    "    new_words=[]\n",
    "    if stop:break\n",
    "    for k in category_tags:\n",
    "        if k in model.wv.vocab.keys():\n",
    "            new_words.extend(model.most_similar(k))\n",
    "    for n in new_words:\n",
    "        if n[0].islower() and len(n[0])>3 and n[0].isalpha():\n",
    "            category_tags.add(n[0])\n",
    "            if len(category_tags)==cate_size:\n",
    "                stop=True\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "category_tags=np.array(list(category_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_count=CountVectorizer(lowercase=False,min_df=1,vocabulary=category_tags)\n",
    "X_ct_count=ct_count.fit_transform(sentences)\n",
    "X_ct_count=X_ct_count.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ct_tfidf=TfidfVectorizer(lowercase=False,min_df=1,vocabulary=category_tags)\n",
    "X_ct_tfidf=ct_tfidf.fit_transform(sentences)\n",
    "X_ct_tfidf=X_ct_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_freq_w_pos=np.array(np.sum(X_ct_count[labels[:,3]==1.0],axis=0)*X_ct_count.shape[0]).reshape(X_ct_count.shape[1],)\n",
    "ct_freq_w_pos[ct_freq_w_pos==0]=1\n",
    "ct_freq_w=np.sum(X_ct_count,axis=0)\n",
    "ct_freq_pos=np.sum(labels[:,3]==1.0)\n",
    "ct_PMI_pos=np.log(ct_freq_w_pos*sentences.shape[0]/ct_freq_w*ct_freq_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_freq_w_neg=np.array(np.sum(X_ct_count[labels[:,3]==-1.0],axis=0)*X_ct_count.shape[0]).reshape(X_ct_count.shape[1],)\n",
    "ct_freq_w_neg[ct_freq_w_neg==0]=1\n",
    "ct_freq_w=np.sum(X_ct_count,axis=0)\n",
    "ct_freq_neg=np.sum(labels[:,3]==-1.0)\n",
    "ct_PMI_neg=np.log(ct_freq_w_neg*sentences.shape[0]/ct_freq_w*freq_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_PS=ct_PMI_pos-ct_PMI_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dict=np.concatenate((bag_of_keywords,category_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dict,full_idx=np.unique(full_dict,return_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_PS=np.concatenate((bok_PS,ct_PS))[full_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tfidf=TfidfVectorizer(lowercase=False,min_df=1,vocabulary=full_dict,use_idf=False)\n",
    "X_full_tfidf=full_tfidf.fit_transform(sentences)\n",
    "X_full_tfidf=X_full_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,h in enumerate(sentence_handler):\n",
    "    for nc in h.noun_chunks:\n",
    "        if labels[i,0] in nc.text or labels[i,1] in nc.text or labels[i,2] in nc.text:\n",
    "            if nc.root.head.text in full_dict:\n",
    "                kwd_idx=np.where(full_dict==nc.root.head.text)[0][0]\n",
    "                if nc.root.dep_ == spacy.symbols.nsubj:\n",
    "                    X_full_tfidf[i,kwd_idx]=X_full_tfidf[i,kwd_idx]*all_PS[kwd_idx]\n",
    "                if nc.root.dep_ == spacy.symbols.nsubjpass:\n",
    "                    X_full_tfidf[i,kwd_idx]=X_full_tfidf[i,kwd_idx]*(-all_PS[kwd_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_price=np.zeros((labels.shape[0],6))\n",
    "for i,pvec in enumerate(labels[:,4]):\n",
    "    X_price[i]=pvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.concatenate((X_full_tfidf,X_price),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36830, 1734)"
      ]
     },
     "execution_count": 729,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array(labels[:,-3],dtype='int32')\n",
    "y[y==1]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=to_categorical(y,num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19605"
      ]
     },
     "execution_count": 733,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y[:,0]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17225"
      ]
     },
     "execution_count": 734,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y[:,1]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29464 samples, validate on 7366 samples\n",
      "Epoch 1/10\n",
      "29464/29464 [==============================] - 17s - loss: 0.6926 - acc: 0.5213 - val_loss: 0.6818 - val_acc: 0.5836\n",
      "Epoch 2/10\n",
      "29464/29464 [==============================] - 17s - loss: 0.6715 - acc: 0.5821 - val_loss: 0.7079 - val_acc: 0.5220\n",
      "Epoch 3/10\n",
      "29464/29464 [==============================] - 17s - loss: 0.6286 - acc: 0.6402 - val_loss: 0.7219 - val_acc: 0.5496\n",
      "Epoch 4/10\n",
      "29464/29464 [==============================] - 18s - loss: 0.5670 - acc: 0.6871 - val_loss: 0.7803 - val_acc: 0.5153\n",
      "Epoch 5/10\n",
      "29464/29464 [==============================] - 18s - loss: 0.4997 - acc: 0.7322 - val_loss: 0.8676 - val_acc: 0.5339\n",
      "Epoch 6/10\n",
      "29464/29464 [==============================] - 17s - loss: 0.4495 - acc: 0.7604 - val_loss: 0.9237 - val_acc: 0.5110\n",
      "Epoch 7/10\n",
      "29464/29464 [==============================] - 18s - loss: 0.4166 - acc: 0.7731 - val_loss: 0.9466 - val_acc: 0.5201\n",
      "Epoch 8/10\n",
      "29464/29464 [==============================] - 17s - loss: 0.3847 - acc: 0.7866 - val_loss: 1.0036 - val_acc: 0.5320\n",
      "Epoch 9/10\n",
      "29464/29464 [==============================] - 18s - loss: 0.3707 - acc: 0.7944 - val_loss: 1.0558 - val_acc: 0.5475\n",
      "Epoch 10/10\n",
      "29464/29464 [==============================] - 17s - loss: 0.3548 - acc: 0.7993 - val_loss: 1.1141 - val_acc: 0.5453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc51a0d4e10>"
      ]
     },
     "execution_count": 738,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnmodel=Sequential()\n",
    "nnmodel.add(Dense(1024,activation='relu',input_dim=X_full_tfidf.shape[1]))\n",
    "nnmodel.add(Dropout(0.5))\n",
    "nnmodel.add(Dense(1024,activation='relu'))\n",
    "nnmodel.add(Dropout(0.5))\n",
    "nnmodel.add(Dense(1024,activation='relu'))\n",
    "nnmodel.add(Dropout(0.5))\n",
    "nnmodel.add(Dense(1024,activation='relu'))\n",
    "nnmodel.add(Dropout(0.5))\n",
    "nnmodel.add(Dense(2,activation='softmax'))\n",
    "nnmodel.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "nnmodel.fit(X_full_tfidf, y,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}